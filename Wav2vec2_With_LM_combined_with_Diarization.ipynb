{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjBvAHgNoR7eGVpPK96EeT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ehsan77e/ASR-with-Diarization/blob/main/Wav2vec2_With_LM_combined_with_Diarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Dependencies </h1>\n",
        "<p>run the cell below, then restart the runtime for them to work</p>\n",
        "if you want to use diarization 3.0, install either onnxruntime or gpu veriation of it"
      ],
      "metadata": {
        "id": "EeF4p6ehdR0c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ipG7qXQc_IM"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install transformers\n",
        "import transformers\n",
        "import torch\n",
        "import librosa\n",
        "!pip install pyctcdecode https://github.com/kpu/kenlm/archive/master.zip # kenlm\n",
        "import IPython.display as display\n",
        "import librosa\n",
        "\n",
        "\n",
        "!pip install -qq https://github.com/pyannote/pyannote-audio/archive/refs/heads/develop.zip   # install pyannote\n",
        "from pyannote.audio import Model, Pipeline\n",
        "\n",
        "# some requirements for diarization 3.0, install one\n",
        "#pip install onnxruntime\n",
        "#pip install onnxruntime-gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> ASR MODEL <h1>\n",
        "<p> the basic class to transcribe audio. keep in mind that we are also using a lm with it that you need to have it before you proceed. check out the kenlm to learn how to make a simple language model.</p>"
      ],
      "metadata": {
        "id": "S51bBZaEagJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ASR:\n",
        "    def __init__(self, model_path, processor_path, computation_method = 'cpu'):\n",
        "        self.model = transformers.Wav2Vec2ForCTC.from_pretrained(model_path).to(torch.device(computation_method))\n",
        "        self.processor = transformers.Wav2Vec2ProcessorWithLM.from_pretrained(processor_path)\n",
        "        self.computation_method = computation_method\n",
        "\n",
        "\n",
        "\n",
        "    def transcribe(self, audio_path, start=0, end=None):\n",
        "        if end == None:         # if start and end are not set by request\n",
        "            speech, rate = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "        else:                   # if start and end are set by request\n",
        "            speech, rate = librosa.load(audio_path, sr=16000, offset=start, duration=end - start)\n",
        "\n",
        "        input_values = self.processor(speech, sampling_rate=16_000, return_tensors='pt').to(torch.device(self.computation_method))\n",
        "        with torch.no_grad():   # the actual computation\n",
        "            logits = self.model(input_values.input_values, attention_mask=input_values.attention_mask).logits\n",
        "\n",
        "        if self.computation_method == \"cpu\":\n",
        "            logits = logits.numpy()  # Convert the tensor to a NumPy array\n",
        "        transcription = self.processor.batch_decode(logits).text\n",
        "        return transcription[0]\n",
        "\n",
        "    def display_audio(self, audio_path):\n",
        "        return display.Audio(audio_path)"
      ],
      "metadata": {
        "id": "XLI796JLX0cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# creating an instance of it"
      ],
      "metadata": {
        "id": "Z3zXYUHwbu1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = 'model_name_or_path'\n",
        "processor_path = 'processor_name_or_path'\n",
        "asr_model = ASR(model_path = model_path, processor_path=processor_path)"
      ],
      "metadata": {
        "id": "KWs0JK-mX-PN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# testing it out on a sample audio"
      ],
      "metadata": {
        "id": "G3ZWhpn4byoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audio_path = 'sample_audio_path'\n",
        "asr_model.transcribe(audio_path)"
      ],
      "metadata": {
        "id": "3zVAyAzUYetS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# displaying the sample audio if you need"
      ],
      "metadata": {
        "id": "dRO3hnWhb9TD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "asr_model.display_audio(audio_path)"
      ],
      "metadata": {
        "id": "c06Q5KyeYhIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# the basic class for a diarization model\n",
        "<p> speaker diarization is a task which recognizes which speaker is speaking at different timestamps</p>"
      ],
      "metadata": {
        "id": "UUd9zB24cBoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Diarization_Model:\n",
        "    def __init__(self, model_path,computation_method = 'cpu', auth_token=None):\n",
        "        if auth_token is None:\n",
        "            self.model = Pipeline.from_pretrained(model_path)\n",
        "            self.model.to(torch.device(computation_method))\n",
        "\n",
        "        else:\n",
        "            self.model = Pipeline.from_pretrained(\n",
        "            model_path,\n",
        "            use_auth_token=auth_token)\n",
        "\n",
        "\n",
        "# ********************************************************************************************************************************************************\n",
        "\n",
        "    def create_spoken_time_periods(self, diarization, speaker_00_name, speaker_01_name):\n",
        "\n",
        "        # the spoken_time_periods is a list, each object of it is also a list, where the first item is start, second is end, and\n",
        "        # the third is the speaker\n",
        "        self.spoken_time_periods = []\n",
        "\n",
        "        for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
        "            if speaker.lower() == 'speaker_00':\n",
        "                self.spoken_time_periods.append({'start': turn.start, 'end': turn.end, 'speaker': speaker_00_name})\n",
        "            else:\n",
        "                self.spoken_time_periods.append({'start': turn.start, 'end': turn.end, 'speaker': speaker_01_name})\n",
        "\n",
        "        self.spoken_time_periods = sorted(self.spoken_time_periods, key=lambda x: x['start'])\n",
        "\n",
        "\n",
        "# ********************************************************************************************************************************************************\n",
        "\n",
        "    def concat_same_speaker(self):\n",
        "        self.spoken_time_periods_concated = []\n",
        "\n",
        "        i = 0 # for i'th item\n",
        "        while i < len(self.spoken_time_periods):\n",
        "            j = 1  # for j'th item in front of the i'th one\n",
        "            still_same_person = True\n",
        "            start = self.spoken_time_periods[i]['start']\n",
        "            end = self.spoken_time_periods[i]['end']\n",
        "            speaker = self.spoken_time_periods[i]['speaker']\n",
        "\n",
        "            while still_same_person:\n",
        "                if i + j < len(self.spoken_time_periods):\n",
        "                    if speaker == self.spoken_time_periods[i+j]['speaker']:\n",
        "                        end = self.spoken_time_periods[i+j]['end']\n",
        "                        j += 1\n",
        "\n",
        "                    else:\n",
        "                        still_same_person = False\n",
        "                        self.spoken_time_periods_concated.append({'start': start, 'end': end, 'speaker': speaker})\n",
        "                        i += j\n",
        "                else:\n",
        "                    still_same_person = False\n",
        "                    self.spoken_time_periods_concated.append({'start': start, 'end': end, 'speaker': speaker})\n",
        "                    i += j\n",
        "\n",
        "\n",
        "# *********************************************** concurrent speaking removed version or \"enhanced version\" **********************************************\n",
        "# it detects how many times each speaker interupted and the overal time they spoke concurrently\n",
        "\n",
        "    def detect_concurrent_speaking(self, speaker_00_name, speaker_01_name,\\\n",
        "                                    word_for_concurrent_speeches, margin_for_interruption):\n",
        "\n",
        "\n",
        "        self.concurrent_speech_time = 0\n",
        "        self.speaker_00_interupts = 0\n",
        "        self.speaker_01_interupts = 0\n",
        "        self.word_for_concurrent_speeches = word_for_concurrent_speeches\n",
        "        self.concurrent_time_periods = []\n",
        "\n",
        "        i = 0 # for i'th speech\n",
        "        while True:\n",
        "\n",
        "            for j in range(i+1, len(self.spoken_time_periods)): # loop over the remaining speeches to detect any concorrent speeches\n",
        "\n",
        "                if self.spoken_time_periods[i]['end'] > self.spoken_time_periods[j]['start'] + margin_for_interruption and \\\n",
        "                self.spoken_time_periods[i]['speaker'] != self.spoken_time_periods[j]['speaker']:   # detecting interruptions\n",
        "\n",
        "                    interrupter = self.spoken_time_periods[j]['speaker'] # detecting the interrupter\n",
        "                    if interrupter == speaker_00_name:\n",
        "                        self.speaker_00_interupts += 1\n",
        "                    else:\n",
        "                        self.speaker_01_interupts += 1\n",
        "\n",
        "                    start_point = self.spoken_time_periods[j]['start'] # the point when they start speaking at the same time\n",
        "                    end_point = min(self.spoken_time_periods[i]['end'], self.spoken_time_periods[j]['end']) # the point when it ends\n",
        "                    self.concurrent_speech_time += end_point - start_point\n",
        "\n",
        "                    self.concurrent_time_periods.append([start_point, end_point])\n",
        "\n",
        "            i += 1\n",
        "            if i == len(self.spoken_time_periods):\n",
        "                break\n",
        "\n",
        "\n",
        "# ***************************************************************************************************************\n",
        "\n",
        "    def diarize(self, audio_path, num_speakers = 2,speaker_00_name = 'speaker_00', speaker_01_name = 'speaker_01',\\\n",
        "                word_for_concurrent_speeches = 'concurrent speaking', margin_for_interruption = 0.10):\n",
        "\n",
        "\n",
        "        diarization = self.model(audio_path, num_speakers=num_speakers)\n",
        "        self.create_spoken_time_periods(diarization, speaker_00_name, speaker_01_name)\n",
        "        self.detect_concurrent_speaking(speaker_00_name, speaker_01_name, word_for_concurrent_speeches, margin_for_interruption)\n",
        "        self.concat_same_speaker()\n",
        "        # self.detect_concurrent_speaking(diarization, speaker_00_name, speaker_01_name,\\\n",
        "        #                                 word_for_concurrent_speeches, margin_for_interruption)\n",
        "\n",
        "        return self.spoken_time_periods,self.spoken_time_periods_concated"
      ],
      "metadata": {
        "id": "2yeLCXSXYk5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# initializing the diarization model"
      ],
      "metadata": {
        "id": "bus38k8Hcl1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = 'pyannote/speaker-diarization-3.0'\n",
        "auth = 'your_auth_token'\n",
        "diarization_model = Diarization_Model(model_path = model_path, auth_token = auth)\n",
        "\n",
        "# you can use code below if you have diarization model mounted on your drive:\n",
        "\n",
        "# model_path = 'model_name_or_path'\n",
        "# diarization_model = Diarization_Model(model_path = model_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "m6eT_5tGYvZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# combining ASR and diarization model"
      ],
      "metadata": {
        "id": "I0z9Ax2dcu9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SpeechMapper:\n",
        "    def __init__(self, asr_model: ASR, diarization_model: Diarization_Model):\n",
        "        self.asr_model = asr_model\n",
        "        self.diarization_model = diarization_model\n",
        "\n",
        "    def transcribe_with_diarization_list(self, audio_path, padding_for_end = 0.1, padding_for_start = 0.1):\n",
        "\n",
        "        # the purpose of this function is to create mapped_transcription, a list where first item is start, second is\n",
        "        # end, third for speaker, forth for transcription\n",
        "        spoken_time_periods, concated_spoken_time_periods = self.diarization_model.diarize(audio_path = audio_path)\n",
        "        audio, sr = librosa.load(audio_path)  # reading the audio file initially to compute its durition\n",
        "\n",
        "        # Get the duration of the audio in seconds\n",
        "        audio_lentgh = librosa.get_duration(y=audio, sr=sr)\n",
        "\n",
        "        self.mapped_transcription = []\n",
        "\n",
        "        for speech in concated_spoken_time_periods:\n",
        "            start = max(speech['start'] - padding_for_start, 0)\n",
        "            end = min(speech['end'] + padding_for_end, audio_lentgh)\n",
        "            speaker = speech['speaker']\n",
        "\n",
        "            if speaker == self.diarization_model.word_for_concurrent_speeches:      # commenting out the conjunctions\n",
        "                transcription = self.asr_model.transcribe(audio_path, start, end)\n",
        "                speech['transcription'] = transcription\n",
        "                #speech.append(\"\")\n",
        "\n",
        "            else:\n",
        "                transcription = self.asr_model.transcribe(audio_path, start, end )\n",
        "                speech['transcription'] = transcription\n",
        "            self.mapped_transcription.append(speech)\n",
        "\n",
        "\n",
        "    def beautified_transcription(self, audio_path):\n",
        "        self.transcribe_with_diarization_list(audio_path)\n",
        "\n",
        "        for speech in self.mapped_transcription:\n",
        "            start = round(speech['start'], 2)\n",
        "            end = round(speech['end'], 2)\n",
        "            speaker = speech['speaker']\n",
        "            transcription = speech['transcription']\n",
        "            if speaker != self.diarization_model.word_for_concurrent_speeches:\n",
        "                print(f'speaker {speaker} from {start} to {end} said:\\n {transcription}' )\n",
        "            else:\n",
        "                #print(f'\\n //  from {start} to {end} the speakers are concurrently speaking // \\n' )\n",
        "\n",
        "                print(f'from {start} to {end} they are concurrently speaking:\\n {transcription}' )\n",
        "\n"
      ],
      "metadata": {
        "id": "29F1QybvY4RN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# creating an instance of if"
      ],
      "metadata": {
        "id": "dUdLGPo5c1ri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "speech_mapper = SpeechMapper(asr_model, diarization_model)"
      ],
      "metadata": {
        "id": "QdPWUrYNZi5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# testing on a sample Audio"
      ],
      "metadata": {
        "id": "kzndtvdqc6Bx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audio_path = 'sample_audio_path'\n",
        "speech_mapper.beautified_transcription(audio_path)"
      ],
      "metadata": {
        "id": "vMpc_u2QZlsP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}